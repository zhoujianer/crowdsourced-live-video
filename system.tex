%!TEX root = main.tex

\section{System Overview and Dataset}
\label{sec:system}

%In this section, we begin with an overview of the architecture of the crowdsourced live video system, and next describe the dataset which we will investigate in the following sections.

%\subsection{Architecture of crowdsourced live video system}
%\label{sub:system-architecture}

Figure~\ref{fig:system} shows the architecture of the crowdsourced live video system, which consists of five components: a unique global controller, front-end servers, distributed storage system, video sharing users and video viewing users. As shown in the figure, global controller decides which front-end server to serve the video sharing or viewing requests. Aditya \etal ~\cite{ganjam2015c3} have proved that a controller can make video broadcast system effective.

Video frames generated by video sharing users are uploaded to storage system via the selected front-end server, using two TCP connections, in which connection 1 is from user to front-end server and connection 2 is from front-end server to storage system. Video viewing users request the desired video from storage system via the selected front-end server using connection 3 and 4. Note that the front-end servers are deployed in different locations and are scheduled according to the policy of the controller, thus the front-end server for sharing of a video may be different from those serving the viewing of that video. The flows amongst front-end servers and storage server traverse internal network with low latency and nearly zero packet loss. In contrast, the flows between front-end servers and users go through Wide Area Network (WAN) or cellar network, which has high packet loss rate and long latency. Thus in this work we mainly focus on analysis of the performance of flows between front-end servers and users.

%We use Cassandra cluster as our storage system. Cassandra is an key-value based open source distributed storage system which has been deployed in many large Internet company, such as Google, Facebook and Amazon~\cite{Beernaert2013cassandra}. It provide high availability and support clusters spanning multiple datacenters. 
\begin{figure}[ht]
	\centering
	\includegraphics[width=2.5in]{system}
	\caption{An overview of the crowdsourced live video system.}
	\label{fig:system} 
	\termspace
\end{figure}

% \textbf{\textit{Post video:}} When consumers want to post a video, camera will first connect to the controller. The controller will select a best front-end server for it according the camera's location and network condition. Ganjam \etal~\cite{ganjam2015c3} find that a centralized control plane can improve the video quality in video broadcast platform. Cameras set up TCP connection 1 with the 'pick up' front-end server. The front-end server play two kinds of role: publisher and subscriber. Publishers receive the video from camera and the subscriber send video to viewers. After receiving the video from camera the publisher will set up TCP connection 2 to send the video to storage servers which is in the same DCN(Data Center Network) with the front-end servers. 

% \textbf{\textit{Watch video:}} When viewers want to watch a video, they also first connect to the controller. The controller will select the nearest front-end server which is also in the same ISP(Internet Service Provider) with consumer. Viewers set up TCP connection 3 with the 'pick up' front-end to request a live video. And then the front-end server will set up  TCP connection 4 to fetch the video from storage server. If the front-end server and video are in the same DCN, it will fetch the video directly. However, if the video is stored in other DCN, the front-end sever will fetch the video from remote DCN. Those DCNs in our system is connected directly by optical fiber which has a large bandwidth and short RTT. The front-end sever will cache the video temporarily that it is playing. When other consumers request the same video, the front-end servers will send back the video that is cached in it which make it unnecessary to fetch from storage servers. Such caching video solution makes our system scalable as even thousands of consumers request a same video, the front-end sever just needs to fetch the video only once.

% In totally, the crowdsourced live video system set up four TCP connections for a single live video watching process. The connections 2 and 4 are between front-end server and storage server and they both go through internal network which has low packet loss rate and short RTT. Connections 1 and 3: camera between front-end sever and front-end sever between viewers go through Wide Area Network (WAN) which has high packet loss rate and long RTT. In Section~\ref{sec:system-measurement} we will analyze the whole system based on the TCP connections, especially the influence caused by the two connections in unstable WAN.

%This is different from traditional live video system. When the traditional live video system broadcast a football game, the camera will transport the video to the front-end server and the front-end server will immediately deliver it to the CDN(Content Dilivery Network) both by stable internal network. As the video is delivered by stable internal netowrk, the traditional live video system has more stable video supply compared with crowdsourced live video system. And that also means that in traditional live video system the network factors that influence user experience is mainly on the connection between Apps and front-end server. However, in crowdsourced live video system the unstable connection between camera and front-end sever may also influence the system. 

% \subsection{Overview of video frame}
% \label{sub:video-frame}

H.264 is used as the video codec in the system, in which there are 4 types of frame: I, P, B and V~\cite{schwarz2007overview}. I frame is the key frame with largest size (10 - 100 Kbytes) and slowest production speed (0.25 f/s). P and B frames (both of which are called P frame for short in the following) are predicted frames with size 2 - 10 Kbytes. The production speed of P frame is 25 f/s and is adaptive according to the network quality. V frame carries voice data with size less than 1 Kbytes and production speed 25 f/s. 

% The video codec in our system is H.264 and we use RTMP(Real Time Messaging Protocol, which is based on TCP) ~\cite{zambelli2009iis} to transfer video. In H.264 there are four kinds of frame to produce a video: I frame, P frame, B frame and V frame. I frame is IDR(Instantaneous Decoder Refresh) frame, which is known as a keyframe and can build a without any other information ~\cite{timothy2011h264}.  P frame is Predicted frame which predicts the picture according the previous frame. B frame is Bidirectional predicted frame which predicts picture according the previous or future frame. In this paper, we do not distinguish P frame and B frame and call them both as P frame. V frame is the voice frame which records the voice along the video. The size and produced terminal of frame are different. I frame is often 10-100K bytes and is produced every 4 seconds. P frame is often 2-10K bytes and its produced terminal is unstable, changed according the dynamic of the video. The size of V frame is less than 1K bytes and our system produces 25 V frames every second. Different frames may have different performance in network. In Section~\ref{sec:frame-analysis} we will detail the different performance of each type of frame.


%For timeless storage and fetching the video, the storage unit in our Cassandra cluster is frame. Which means the system uses a key to storage and fetch echa frame.  

\iffalse
\begin{figure}[ht]
	\centering
	\includegraphics[width=2.5in]{frame}
	\caption{An overview of H.264 frame.}
	\label{fig:frame}
	\termspace
\end{figure}
\fi

The packet-level dataset we use for analysis was collected via tcpdump in one front-end server, which spans from Jul 15th, 2015 to Jul 22nd 2015. We captured all the video flows that go through the front-end servers. Overall, we collected 4.88 billion packets, corresponding to 1.24 million flows. The average packet size is 1028 bytes and average flow size is 3.86MB. Through examining the TCP ports that each packet is assigned, we could distinguish which one of the four connections (flows) the packet belongs. 